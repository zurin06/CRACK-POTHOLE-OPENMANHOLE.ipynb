# -*- coding: utf-8 -*-
"""CRACK,POTHOLE,OPENMANHOLE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/crack-pothole-openmanhole-3e964139-3ff4-414f-bf62-e6d90e8fc39f.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250220/auto/storage/goog4_request%26X-Goog-Date%3D20250220T192111Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D45e5b55d9829420d5a3dd9aeb38b851a40250aec508199a82fe43d2d1ef30dd6d8af6505a3db1ac778935b91ee4f6831fe3bc5ebc626d783523ac955a30143e1d1218e680032a7da5c64f6651fd5753451e55bade3a39af61505cbd4d0ad49f78adf2c71d69cfda8994396a726f2bd2da139f60843d037ba2d891d2d174a9922c7e957cc9607e002f84b49bcc5a08ab06b1807a9c5a028c508d7396209596bc469ed7beeb529d013b3f3de5accee7631c1a80665234a2fc0f23d93cc227a020ebe8fb6bcbd024246facbc766f87c2f6fec79082edac025075c63276b49fd62c0f6323955102b41b0fdc9fca9c03380099d9682ec6d8a8ecd615f8f3065c0070a
"""

# Install common dependencies
!pip install torch torchvision torchaudio
!pip install opencv-python-headless
!pip install matplotlib
!pip install tqdm
!pip install PyYAML
!pip install seaborn
!pip install pandas
!pip install scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# Clone the YOLOv8 repository
!git clone https://github.com/ultralytics/ultralytics
# %cd ultralytics

# Install dependencies manually
!pip install torch torchvision torchaudio
!pip install opencv-python-headless
!pip install matplotlib
!pip install tqdm
!pip install PyYAML
!pip install seaborn
!pip install pandas
!pip install scikit-learn

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Verify the contents of your drive
!ls /content/drive/MyDrive/data

!pip install ultralytics

import torch
from ultralytics import YOLO

# Load the YOLOv8s model
model = YOLO('/content/ultralytics/yolov8n.pt')  # You can choose other versions like yolov8n.pt, yolov8m.pt, etc.

# Train the model
model.train(data='/content/drive/MyDrive/data/data.yaml', epochs=3, imgsz=640, batch=16)

import os

def check_labels(image_dir, label_dir):
    missing_labels = []
    for image_file in os.listdir(image_dir):
        label_file = os.path.splitext(image_file)[0] + '.txt'
        if not os.path.exists(os.path.join(label_dir, label_file)):
            missing_labels.append(image_file)
    return missing_labels

image_dir = '/content/drive/MyDrive/data/dataset/classes/cracks/images'
label_dir = '/content/drive/MyDrive/data/dataset/classes/cracks/labels'
missing_labels = check_labels(image_dir, label_dir)

if missing_labels:
    print("Missing labels for the following images:")
    for image in missing_labels:
        print(image)
else:
    print("All images have corresponding labels.")

import matplotlib.pyplot as plt

# Example metrics data (replace with your actual metrics)
metrics = {
    'epochs': [1, 2],
    'loss': [0.0, 0.0],  # Example data
    'precision': [0.3, 0.35],
    'recall': [0.25, 0.3],
    'mAP50': [0.2, 0.25]
}

# Plotting the metrics
plt.figure(figsize=(12, 6))

plt.subplot(2, 2, 1)
plt.plot(metrics['epochs'], metrics['loss'], label='Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()

plt.subplot(2, 2, 2)
plt.plot(metrics['epochs'], metrics['precision'], label='Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.title('Precision')
plt.legend()

plt.subplot(2, 2, 3)
plt.plot(metrics['epochs'], metrics['recall'], label='Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.title('Recall')
plt.legend()

plt.subplot(2, 2, 4)
plt.plot(metrics['epochs'], metrics['mAP50'], label='mAP@50')
plt.xlabel('Epochs')
plt.ylabel('mAP@50')
plt.title('mAP@50')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate the model on the validation set
metrics = model.val(data='/content/drive/MyDrive/data/data.yaml', imgsz=640)

# Evaluate the model on the test set
metrics = model.val(data='/content/drive/MyDrive/data/data.yaml', imgsz=640, split='test')

import matplotlib.pyplot as plt
import cv2
import os

# Directory to save annotated images
output_dir = '/content/drive/MyDrive/data/predicted'
os.makedirs(output_dir, exist_ok=True)

# Run inference on a directory of images with adjusted IoU threshold for NMS
results = model.predict(source='/content/drive/MyDrive/data/dataset/classes/pothole/images', imgsz=640, conf=0.25, iou=0.3)

# Initialize counters for evaluation metrics
total_images = len(results)
correct_detections = 0
false_positives = 0
false_negatives = 0

# Loop through the results and display each image with annotations
for i, result in enumerate(results):
    # Get the annotated image
    annotated_img = result.plot()

    # Save the annotated image
    output_path = os.path.join(output_dir, f'annotated_image_{i}.jpg')
    cv2.imwrite(output_path, annotated_img)

    # Display the image with annotations
    plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

    # Evaluate detections (example logic, adjust as needed)
    for detection in result.boxes:
        class_id = detection.cls
        confidence = detection.conf
        if confidence > 0.5:  # Threshold for considering a detection as correct
            correct_detections += 1
        else:
            false_positives += 1

    # Example logic for false negatives (adjust as needed)
    # Assuming you have ground truth labels to compare with
    # false_negatives += number_of_missed_detections

# Calculate and print evaluation metrics
precision = correct_detections / (correct_detections + false_positives) if (correct_detections + false_positives) > 0 else 0
recall = correct_detections / (correct_detections + false_negatives) if (correct_detections + false_negatives) > 0 else 0
f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

print(f'Total Images: {total_images}')
print(f'Correct Detections: {correct_detections}')
print(f'False Positives: {false_positives}')
print(f'False Negatives: {false_negatives}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1_score:.2f}')

import matplotlib.pyplot as plt
import cv2
import os

# Directory to save annotated images
output_dir = '/content/drive/MyDrive/data/predicted'
os.makedirs(output_dir, exist_ok=True)

# Run inference on a directory of images with adjusted IoU threshold for NMS
results = model.predict(source='/content/drive/MyDrive/data/dataset/classes/cracks/images', imgsz=640, conf=0.25, iou=0.3)

# Initialize counters for evaluation metrics
total_images = len(results)
correct_detections = 0
false_positives = 0
false_negatives = 0

# Loop through the results and display each image with annotations
for i, result in enumerate(results):
    # Get the annotated image
    annotated_img = result.plot()

    # Save the annotated image
    output_path = os.path.join(output_dir, f'annotated_image_{i}.jpg')
    cv2.imwrite(output_path, annotated_img)

    # Display the image with annotations
    plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

    # Evaluate detections (example logic, adjust as needed)
    for detection in result.boxes:
        class_id = detection.cls
        confidence = detection.conf
        if confidence > 0.5:  # Threshold for considering a detection as correct
            correct_detections += 1
        else:
            false_positives += 1

    # Example logic for false negatives (adjust as needed)
    # Assuming you have ground truth labels to compare with
    # false_negatives += number_of_missed_detections

# Calculate and print evaluation metrics
precision = correct_detections / (correct_detections + false_positives) if (correct_detections + false_positives) > 0 else 0
recall = correct_detections / (correct_detections + false_negatives) if (correct_detections + false_negatives) > 0 else 0
f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

print(f'Total Images: {total_images}')
print(f'Correct Detections: {correct_detections}')
print(f'False Positives: {false_positives}')
print(f'False Negatives: {false_negatives}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1_score:.2f}')

import matplotlib.pyplot as plt
import cv2
import os

# Directory to save annotated images
output_dir = '/content/drive/MyDrive/data/predicted'
os.makedirs(output_dir, exist_ok=True)

# Run inference on a directory of images with adjusted IoU threshold for NMS
results = model.predict(source='/content/drive/MyDrive/data/dataset/classes/open_manhole/images', imgsz=640, conf=0.25, iou=0.3)

# Initialize counters for evaluation metrics
total_images = len(results)
correct_detections = 0
false_positives = 0
false_negatives = 0

# Loop through the results and display each image with annotations
for i, result in enumerate(results):
    # Get the annotated image
    annotated_img = result.plot()

    # Save the annotated image
    output_path = os.path.join(output_dir, f'annotated_image_{i}.jpg')
    cv2.imwrite(output_path, annotated_img)

    # Display the image with annotations
    plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

    # Evaluate detections (example logic, adjust as needed)
    for detection in result.boxes:
        class_id = detection.cls
        confidence = detection.conf
        if confidence > 0.5:  # Threshold for considering a detection as correct
            if class_id == 1:  # Assuming class_id 1 corresponds to cracks
                correct_detections += 1
            else:
                false_positives += 1

    # Example logic for false negatives (adjust as needed)
    # Assuming you have ground truth labels to compare with
    # false_negatives += number_of_missed_detections

# Calculate and print evaluation metrics
precision = correct_detections / (correct_detections + false_positives) if (correct_detections + false_positives) > 0 else 0
recall = correct_detections / (correct_detections + false_negatives) if (correct_detections + false_negatives) > 0 else 0
f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

print(f'Total Images: {total_images}')
print(f'Correct Detections: {correct_detections}')
print(f'False Positives: {false_positives}')
print(f'False Negatives: {false_negatives}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1_score:.2f}')



import matplotlib.pyplot as plt

# Example metrics data (replace with your actual metrics)
metrics = {
    'epochs': [1, 2],
    'mAP50': [0.497 , 0.25],  # Example data
    'mAP50-95': [ 0.208, 0.15]
}


# Plotting the mAP metrics
plt.figure(figsize=(10, 5))

plt.plot(metrics['epochs'], metrics['mAP50'], label='mAP@50')
plt.plot(metrics['epochs'], metrics['mAP50-95'], label='mAP@50-95')
plt.xlabel('Epochs')
plt.ylabel('mAP')
plt.title('mAP Metrics Over Epochs')
plt.legend()
plt.show()